{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in“Bangalore” location. You have to scrape the job-title, job-location, company_name,experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore”\n",
    "in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchjob=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "searchjob.send_keys('Data Analyst')\n",
    "searchloc=driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "searchloc.send_keys('Bangalore')\n",
    "searchbtn=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "searchbtn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "locations=[]\n",
    "company=[]\n",
    "experience=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "job[0:10]\n",
    "\n",
    "for i in job:\n",
    "    jobs=i.text\n",
    "    job_title.append(jobs)\n",
    "job_title[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "area[0:10]\n",
    "\n",
    "for i in area:\n",
    "    location=i.text\n",
    "    locations.append(location)\n",
    "locations[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "com[0:10]\n",
    "\n",
    "for i in com:\n",
    "    comp=i.text\n",
    "    company.append(comp)\n",
    "company[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "ex[0:10]\n",
    "\n",
    "for i in ex:\n",
    "    exp=i.text\n",
    "    experience.append(exp)\n",
    "experience[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_title[0:10]\n",
    "jobs['location']=locations[0:10]\n",
    "jobs['company']=company[0:10]\n",
    "jobs['experience']=experience[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in“Bangalore” location. You have to scrape the job-title, job-location,company_name, full job-description. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter\n",
    "“Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- 1. All of the above steps have to be done in code. No step is to be done\n",
    "manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchjob=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "searchjob.send_keys('Data Scientist')\n",
    "searchloc=driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "searchloc.send_keys('Bangalore')\n",
    "searchbtn=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "searchbtn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.naukri.com/data-scientist-jobs-in-bangalore?k=data%20scientist&l=bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\"):\n",
    "    urls.append(i.get_attribute('href'))\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "locations=[]\n",
    "company=[]\n",
    "job_des=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        des=driver.find_element_by_xpath(\"//div[@class='dang-inner-html']\")\n",
    "        job_des.append(des.text)\n",
    "    except:\n",
    "        job_des.append('-')\n",
    "\n",
    "    try:\n",
    "        job=driver.find_element_by_xpath(\"//div[@class='jd-top-head']\")\n",
    "        job_title.append(job.text.replace(\"/n\",\"new line\"))\n",
    "    except:\n",
    "        job_title.append('-')\n",
    "        \n",
    "    try:\n",
    "        loc=driver.find_element_by_xpath(\"//div[@class='loc']/span/a[1]\")\n",
    "        locations.append(loc.text)\n",
    "    except:\n",
    "        locations.append('-')\n",
    "        \n",
    "    try:\n",
    "        com=driver.find_element_by_path(\"//div[@class='jd-header-comp-name']/a[1]\")\n",
    "        company.append(com.text)\n",
    "    except:\n",
    "            company.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_des))\n",
    "print(len(job_title))\n",
    "print(len(locations))\n",
    "print(len(company))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_des[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_title[0:11]\n",
    "jobs['location']=locations[0:11]\n",
    "jobs['company']=company[0:11]\n",
    "jobs['description']=job_des[0:11]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on thewebpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company_name,\n",
    "experience_required.\n",
    "The location filter to be used is “Delhi/NCR”\n",
    "The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field .\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchjob=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "searchjob.send_keys('Data Scientist')\n",
    "searchbtn=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "searchbtn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.naukri.com/data-scientist-jobs?k=data%20scientist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city=driver.find_elements_by_xpath(\"//span[@class='ellipsis fleft']\")\n",
    "\n",
    "for i in city:\n",
    "    if i.text=='Delhi / NCR':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package=driver.find_elements_by_xpath(\"//span[@class='ellipsis fleft']\")\n",
    "\n",
    "for i in package:\n",
    "    if i.text=='3-6 Lakhs':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]\n",
    "location=[]\n",
    "company=[]\n",
    "experience=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "a[0:10]\n",
    "\n",
    "for i in a:\n",
    "    ttl=i.text\n",
    "    title.append(ttl)\n",
    "title[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "b[0:10]\n",
    "\n",
    "for i in b:\n",
    "    loc=i.text\n",
    "    location.append(loc)\n",
    "location[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=driver.find_elements_by_xpath(\"//div[@class='mt-7 companyInfo subheading lh16']/a[1]\")\n",
    "c[0:10]\n",
    "\n",
    "for i in c:\n",
    "    com=i.text\n",
    "    company.append(com)\n",
    "company[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\")\n",
    "d[0:10]\n",
    "\n",
    "for i in d:\n",
    "    exp=i.text\n",
    "    experience.append(exp)\n",
    "experience[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs=pd.DataFrame({})\n",
    "jobs['job-title']=title[0:10]\n",
    "jobs['job-location']=location[0:10]\n",
    "jobs['company']=company[0:10]\n",
    "jobs['experience']=experience[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida”\n",
    "in “location” field.\n",
    "3. Then click the search button. You will land up in the below page:\n",
    "4. Then scrape the data for the first 10 jobs results you get in the above shown\n",
    "page.\n",
    "WEB SCRAPING ASSIGNMENT-2\n",
    ".\n",
    "5. Finally create a dataframe of the scraped data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.glassdoor.co.in/Job/Home/recentActivity.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchjob=driver.find_elements_by_xpath(\"//input[@id='sc.keyword']\")\n",
    "searchjob.insert('Data Scientist')\n",
    "searchloc=driver.find_elements_by_xpath(\"//input[@id='sc.location']\")\n",
    "searchloc.insert('Noida')\n",
    "searchbtn=driver.find_elements_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "searchbtn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location. You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "2. Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "3. Click the search button.\n",
    "4. After that you will land on the below page\n",
    "You have to scrape whole data from this webpage\n",
    "5. Scrape data for first 10 companies. Scrape the min salary, max salary, company\n",
    "name, Average salary and rating of the company.\n",
    "6.Store the data in a dataframe.\n",
    "Note that all of the above steps have to be done by coding only and not manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have toscrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and\n",
    "more” is written and click the search icon\n",
    "3. after that you will reach to a webpage having a lot of sunglasses. From this page\n",
    "you can scrap the required data as usual.\n",
    "4. after scraping data from the first page, go to the “Next” Button at the bottom of\n",
    "the page , then click on it\n",
    "WEB SCRAPING ASSIGNMENT-2\n",
    ".\n",
    "5. Now scrape data from this page as usual\n",
    "6. repeat this until you get data for 100 sunglasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelbtn=driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "cancelbtn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']/input[1]\")\n",
    "search.send_keys('Sunglasses')\n",
    "searchbtn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "searchbtn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.flipkart.com/search?q=Sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand=[]\n",
    "discount=[]\n",
    "descriptions=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "a[0:100]\n",
    "for i in a :\n",
    "    br=i.text\n",
    "    brand.append(br)\n",
    "brand[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']/span\")\n",
    "b[0:100]\n",
    "for i in b:\n",
    "    dis=i.text\n",
    "    discount.append(dis)\n",
    "discount[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "d[0:100]\n",
    "for i in d:\n",
    "    de=i.text\n",
    "    descriptions.append(de)\n",
    "descriptions[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "p[0:100]\n",
    "for i in p:\n",
    "    pr=i.text\n",
    "    price.append(pr)\n",
    "price[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunglass=pd.DataFrame({})\n",
    "sunglass['brands']=brand[0:100]\n",
    "sunglass['discounts']=discount[0:100]\n",
    "sunglass['description']=descriptions[0:100]\n",
    "sunglass['pricing']=price[0:100]\n",
    "sunglass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. \n",
    "You have togo the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includesearpods-poweradapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\n",
    "When you will open the above link you will reach to the below shown webpage.\n",
    "As shown in the above page you have to scrape the tick marked attributes.\n",
    "These are\n",
    "1. Rating\n",
    "2. Review_summary\n",
    "3. Full review\n",
    "You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb-includesearpods-poweradapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchbtn=driver.find_element_by_xpath(\"//div[@class='_3UAT2v _16PBlm']/span\")\n",
    "searchbtn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating=[]\n",
    "review=[]\n",
    "summary=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "a[0:100]\n",
    "\n",
    "for i in a:\n",
    "    rat=i.text\n",
    "    rating.append(rat)\n",
    "rating[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "b[0:100]\n",
    "\n",
    "for i in b:\n",
    "    rev=i.text\n",
    "    review.append(rev)\n",
    "review[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "c[0:100]\n",
    "\n",
    "for i in c:\n",
    "    sum=i.text\n",
    "    summary.append(sum)\n",
    "summary[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile=pd.DataFrame({})\n",
    "mobile['ratings']=rating[0:100]\n",
    "mobile['review']=review[0:100]\n",
    "mobile['summary']=summary[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    " You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %\n",
    "As shown in the below image, you have to scrape the tick marked attributes.\n",
    "\n",
    "Also note that all the steps required during scraping should be done through code\n",
    "only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelbtn=driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "cancelbtn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']/input[1]\")\n",
    "search.send_keys('sneakers')\n",
    "searchbtn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "searchbtn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand=[]\n",
    "discount=[]\n",
    "descriptions=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "a[0:100]\n",
    "for i in a:\n",
    "    br=i.text\n",
    "    brand.append(br)\n",
    "brand[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "b[0:100]\n",
    "for i in b:\n",
    "    dis=i.text\n",
    "    discount.append(dis)\n",
    "discount[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "c[0:100]\n",
    "for i in c:\n",
    "    dis=i.text\n",
    "    descriptions.append(dis)\n",
    "descriptions[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "p[0:100]\n",
    "for i in p:\n",
    "    pr=i.text\n",
    "    price.append(pr)\n",
    "price[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sneaker=pd.DataFrame({})\n",
    "sneaker['brands']=brand[0:100]\n",
    "sneaker['discount']=discount[0:100]\n",
    "sneaker['description']=descriptions[0:100]\n",
    "sneaker['price']=price[0:100]\n",
    "sneaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, as shown in\n",
    "the below image\n",
    ".\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of\n",
    "the shoes , Short Shoe description, price of the shoe as shown in the below image.\n",
    "Please note that applying the filter and scraping the data , everything should be\n",
    "done through code only and there should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "url='https://www.myntra.com/shoes'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color=driver.find_elements_by_xpath(\"//span[@class='colour-label colour-colorDisplay']\")\n",
    "\n",
    "for i in color:\n",
    "    if i.text=='Black':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr=driver.find_elements_by_xpath(\"//label[@class='common-customCheckbox vertical-filters-label']/input\")\n",
    "\n",
    "for i in pr:\n",
    "    if i.text=='6649 to Rs. 13099':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.myntra.com/shoes?f=Color%3ABlack_36454f&rf=Price%3A6908.0_13602.0_6908.0%20TO%2013602.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand=[]\n",
    "description=[]\n",
    "prices=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/h3\")\n",
    "b[0:100]\n",
    "for i in b:\n",
    "    br=i.text\n",
    "    brand.append(br)\n",
    "brand[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/h4\")\n",
    "d[0:100]\n",
    "for i in d:\n",
    "    des=i.text\n",
    "    description.append(des)\n",
    "description[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pri=driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "pri[0:100]\n",
    "for i in pri:\n",
    "    p=i.text\n",
    "    prices.append(p)\n",
    "prices[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoes=pd.DataFrame({})\n",
    "shoes['brand']=brand[0:50]\n",
    "shoes['description']=description[0:50]\n",
    "shoes['prices']=prices[0:50]\n",
    "shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10: Go to webpage https://www.amazon.in/\n",
    " Enter “Laptop” in the search field and then click the search icon.\n",
    " Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the\n",
    "below image:\n",
    "WEB SCRAPING ASSIGNMENT-2\n",
    ".\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes\n",
    "for each laptop:\n",
    "1. title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.get('https://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchjob=driver.find_element_by_id('twotabsearchtextbox')\n",
    "searchjob.send_keys('Laptop')\n",
    "searchbtn=driver.find_element_by_xpath(\"//input[@id='nav-search-submit-button']\")\n",
    "searchbtn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.amazon.in/s?k=Laptop&ref=nb_sb_noss_2'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "\n",
    "for i in proc:\n",
    "    if i.text=='Intel Core i7':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procc=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "\n",
    "for i in procc:\n",
    "    if i.text=='Intel Core i9':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]\n",
    "rating=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "t[0:10]\n",
    "for i in t:\n",
    "    ti=i.text\n",
    "    title.append(ti)\n",
    "title[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra=driver.find_elements_by_xpath(\"//span[@class='a-size-base']\")\n",
    "ra[0:10]\n",
    "for i in ra:\n",
    "    tar=i.text\n",
    "    rating.append(tar)\n",
    "rating[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "p[0:10]\n",
    "for i in p:\n",
    "    pr=i.text\n",
    "    price.append(pr)\n",
    "price[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop=pd.DataFrame({})\n",
    "laptop['title']=title[0:10]\n",
    "laptop['rating']=rating[0:10]\n",
    "laptop['price']=price[0:10]\n",
    "laptop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
